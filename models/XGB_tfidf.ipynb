{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/ericxu10101/quora-question-pairs/xgboost-tfidf-logloss-0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv', nrows = 50000)\n",
    "df_train = df_train.fillna(' ')\n",
    "# df_test = pd.read_csv('../data/test.csv', nrows = 10000)\n",
    "# df_test = df_test.fillna(' ')\n",
    "\n",
    "# explore\n",
    "train_qs = pd.Series(df_train['question1'].tolist() + df_train['question2'].tolist()).astype(str)\n",
    "# test_qs = pd.Series(df_test['question1'].tolist() + df_test['question2'].tolist()).astype(str)\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def jaccard(row):\n",
    "    wic = set(row['question1']).intersection(set(row['question2']))\n",
    "    uw = set(row['question1']).union(row['question2'])\n",
    "    if len(uw) == 0:\n",
    "        uw = [1]\n",
    "    return (len(wic) / len(uw))\n",
    "\n",
    "def wc_diff(row):\n",
    "    return abs(len(row['question1']) - len(row['question2']))\n",
    "\n",
    "def wc_diff_unique(row):\n",
    "    return abs(len(set(row['question1'])) - len(set(row['question2'])))\n",
    "\n",
    "def wc_diff_unique_stop(row):\n",
    "    return abs(len([x for x in set(row['question1']) if x not in stops]) - len([x for x in set(row['question2']) if x not in stops]))\n",
    "\n",
    "def same_start_word(row):\n",
    "    return int(row['question1'][0] == row['question2'][0])\n",
    "\n",
    "def char_diff(row):\n",
    "    return abs(len(''.join(row['question1'])) - len(''.join(row['question2'])))\n",
    "\n",
    "def char_diff_unique_stop(row):\n",
    "    return abs(len(''.join([x for x in set(row['question1']) if x not in stops])) - len(''.join([x for x in set(row['question2']) if x not in stops])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "    \n",
    "def tfidf_word_match_share_stops(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in row['question1']:\n",
    "        q1words[word] = 1\n",
    "    for word in row['question2']:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['question1'] = df_train['question1'].map(lambda x: str(x).lower().split())\n",
    "df_train['question2'] = df_train['question2'].map(lambda x: str(x).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# df_test['question1'] = df_test['question1'].map(lambda x: str(x).lower().split())\n",
    "# df_test['question2'] = df_test['question2'].map(lambda x: str(x).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_features(data):\n",
    "    X = pd.DataFrame()\n",
    "    X['word_match'] = data.apply(word_match_share, axis=1, raw=True)\n",
    "    X['tfidf_wm'] = data.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "    X['tfidf_wm_stops'] = data.apply(tfidf_word_match_share_stops, axis=1, raw=True)\n",
    "    X['jaccard'] = data.apply(jaccard, axis=1, raw=True)\n",
    "    X['wc_diff'] = data.apply(wc_diff, axis=1, raw=True)\n",
    "    X['wc_diff_unique'] = data.apply(wc_diff_unique, axis=1, raw=True)\n",
    "    X['wc_diff_unq_stop'] = data.apply(wc_diff_unique_stop, axis=1, raw=True)\n",
    "    X['same_start'] = data.apply(same_start_word, axis=1, raw=True)\n",
    "    X['char_diff'] = data.apply(char_diff, axis=1, raw=True)\n",
    "    X['char_diff_unq_stop'] = data.apply(char_diff_unique_stop, axis=1, raw=True)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atakata\\AppData\\Local\\Continuum\\Anaconda2\\envs\\py3\\lib\\site-packages\\ipykernel\\__main__.py:23: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "# x_train = pd.DataFrame()\n",
    "# x_train['word_match'] = df_train.apply(word_match_share, axis=1, raw=True)\n",
    "# x_train['tfidf_wm'] = df_train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "# x_train['tfidf_wm_stops'] = df_train.apply(tfidf_word_match_share_stops, axis=1, raw=True)\n",
    "# x_train['jaccard'] = df_train.apply(jaccard, axis=1, raw=True)\n",
    "# x_train['wc_diff'] = df_train.apply(wc_diff, axis=1, raw=True)\n",
    "# x_train['wc_diff_unique'] = df_train.apply(wc_diff_unique, axis=1, raw=True)\n",
    "# x_train['wc_diff_unq_stop'] = df_train.apply(wc_diff_unique_stop, axis=1, raw=True)\n",
    "# x_train['same_start'] = df_train.apply(same_start_word, axis=1, raw=True)\n",
    "# x_train['char_diff'] = df_train.apply(char_diff, axis=1, raw=True)\n",
    "\n",
    "x_train = build_features(df_train)\n",
    "y_train = df_train['is_duplicate'].values\n",
    "\n",
    "# x_test = build_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_train = x_train[y_train == 1]\n",
    "neg_train = x_train[y_train == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we oversample the negative class\n",
    "# There is likely a much more elegant way to do this...\n",
    "p = 0.165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18649\n",
      "31351\n",
      "0.37298\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_train))\n",
    "print(len(neg_train))\n",
    "print(len(pos_train)/(len(pos_train)+len(neg_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19091346498367168\n"
     ]
    }
   ],
   "source": [
    "scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "while scale > 1:\n",
    "    neg_train = pd.concat([neg_train, neg_train])\n",
    "    scale -=1\n",
    "neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "print(len(pos_train) / (len(pos_train) + len(neg_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = pd.concat([pos_train, neg_train])\n",
    "y_train = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "# del pos_train, neg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "params['subsample'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.682856\tvalid-logloss:0.683037\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[50]\ttrain-logloss:0.438653\tvalid-logloss:0.443481\n",
      "[100]\ttrain-logloss:0.375683\tvalid-logloss:0.382171\n",
      "[150]\ttrain-logloss:0.355211\tvalid-logloss:0.36247\n",
      "[200]\ttrain-logloss:0.347369\tvalid-logloss:0.355148\n",
      "[250]\ttrain-logloss:0.343703\tvalid-logloss:0.351883\n",
      "[300]\ttrain-logloss:0.341348\tvalid-logloss:0.350064\n",
      "[350]\ttrain-logloss:0.339555\tvalid-logloss:0.348832\n",
      "[400]\ttrain-logloss:0.337756\tvalid-logloss:0.347687\n",
      "[450]\ttrain-logloss:0.33587\tvalid-logloss:0.346489\n",
      "[500]\ttrain-logloss:0.334274\tvalid-logloss:0.345533\n",
      "[550]\ttrain-logloss:0.332916\tvalid-logloss:0.344741\n",
      "[600]\ttrain-logloss:0.331618\tvalid-logloss:0.343976\n",
      "[650]\ttrain-logloss:0.330384\tvalid-logloss:0.34324\n",
      "[700]\ttrain-logloss:0.329303\tvalid-logloss:0.342652\n",
      "[750]\ttrain-logloss:0.328239\tvalid-logloss:0.342101\n",
      "[800]\ttrain-logloss:0.327186\tvalid-logloss:0.341593\n",
      "[850]\ttrain-logloss:0.326138\tvalid-logloss:0.341116\n",
      "[900]\ttrain-logloss:0.325141\tvalid-logloss:0.340628\n",
      "[950]\ttrain-logloss:0.324203\tvalid-logloss:0.340153\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, verbose_eval=50)\n",
    "\n",
    "# d_test = xgb.DMatrix(x_test)\n",
    "# p_test = bst.predict(d_test)\n",
    "# sub = pd.DataFrame()\n",
    "# sub['test_id'] = df_test['test_id']\n",
    "# sub['is_duplicate'] = p_test\n",
    "\n",
    "# print(sub.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_diff': 995,\n",
       " 'char_diff_unq_stop': 823,\n",
       " 'jaccard': 1805,\n",
       " 'same_start': 254,\n",
       " 'tfidf_wm': 3450,\n",
       " 'tfidf_wm_stops': 2544,\n",
       " 'wc_diff': 593,\n",
       " 'wc_diff_unique': 380,\n",
       " 'wc_diff_unq_stop': 402,\n",
       " 'word_match': 2298}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_y = bst.predict(d_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_y > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33962351851029654"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_valid, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81819112453293752"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_valid, (pred_y > 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(50k, 1000 rounds)\n",
    "\n",
    "* Original Rebalanced - .355\n",
    "* O + jacc_nostpo - .348\n",
    "* O + jacc + wc_diffs - .344\n",
    "* O + jacc + wcd + tf_stp - .340\n",
    "* O + jacc + wcd + tf_stp + char_sp - .340\n",
    "\n",
    "(50k, 1000 rounds, 0.5 subsample)\n",
    "* O + jacc + wc_diffs - .343\n",
    "\n",
    "(50k, 1000 rounds, 1.0 subsample, max_depth 5)\n",
    "* O + jacc + wc_diffs - .341\n",
    "\n",
    "(50k, 1000 rounds, no rebalance)\n",
    "* Original - .457\n",
    "* O + jacc + wc_diffs - .441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45881359270459127"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss([0]*83 + [1]*17, [0.2] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64744663903463229"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss([0]*65 + [1]*35, [0.35] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['objective'] = ('binary:logistic',)\n",
    "# params['eval_metric'] = ('logloss',)\n",
    "params['n_estimators'] = (1000)\n",
    "# params['early_stopping_rounds'] = (50,)\n",
    "params['learning_rate'] = (0.02, 0.05)\n",
    "params['max_depth'] = (3,4,5)\n",
    "params['subsample'] = (0.4, 0.6, 0.8, 1.0)\n",
    "params['silent'] = (False, )\n",
    "# params['dtrain'] = (d_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:  {'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.4}  MeanTrainScore -0.607365356421  MeanTestScore:  -0.607459703669\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.6}  MeanTrainScore -0.607327092703  MeanTestScore:  -0.607432086984\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.8}  MeanTrainScore -0.607255700523  MeanTestScore:  -0.607353256244\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 1.0}  MeanTrainScore -0.607249691268  MeanTestScore:  -0.607361253471\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.4}  MeanTrainScore -0.606367039733  MeanTestScore:  -0.606509784881\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.6}  MeanTrainScore -0.606327841497  MeanTestScore:  -0.606486734568\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.8}  MeanTrainScore -0.606222112764  MeanTestScore:  -0.606358186756\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 1.0}  MeanTrainScore -0.606219396072  MeanTestScore:  -0.606340245539\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.4}  MeanTrainScore -0.605700766836  MeanTestScore:  -0.60593929692\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.6}  MeanTrainScore -0.605566584117  MeanTestScore:  -0.605832242575\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.8}  MeanTrainScore -0.605428893103  MeanTestScore:  -0.605722334318\n",
      "Params:  {'learning_rate': 0.02, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 1.0}  MeanTrainScore -0.605434412026  MeanTestScore:  -0.605730101801\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.4}  MeanTrainScore -0.522288862654  MeanTestScore:  -0.522453278084\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.6}  MeanTrainScore -0.522162202279  MeanTestScore:  -0.522378024101\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.8}  MeanTrainScore -0.522074575412  MeanTestScore:  -0.522297798158\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 1.0}  MeanTrainScore -0.522072065168  MeanTestScore:  -0.522297477836\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.4}  MeanTrainScore -0.520120240123  MeanTestScore:  -0.520440925217\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.6}  MeanTrainScore -0.520061175098  MeanTestScore:  -0.520459489628\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.8}  MeanTrainScore -0.519893219899  MeanTestScore:  -0.52026276186\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 1.0}  MeanTrainScore -0.519911188708  MeanTestScore:  -0.520290712177\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.4}  MeanTrainScore -0.518626199081  MeanTestScore:  -0.519247265112\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.6}  MeanTrainScore -0.518410397322  MeanTestScore:  -0.519078217426\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 0.8}  MeanTrainScore -0.51818652633  MeanTestScore:  -0.518890001509\n",
      "Params:  {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 10, 'objective': 'binary:logistic', 'silent': False, 'subsample': 1.0}  MeanTrainScore -0.518237363764  MeanTestScore:  -0.518935174444\n"
     ]
    }
   ],
   "source": [
    "xb = XGBClassifier()\n",
    "gs = GridSearchCV(xb, params, scoring='neg_log_loss', cv=2) #cv=5)\n",
    "gs.fit(x_train, y_train)\n",
    "# gs.fit()\n",
    "results = gs.cv_results_\n",
    "for p, t_v, v_v in zip(results['params'], results['mean_train_score'], results['mean_test_score']):\n",
    "    print('Params: ', p, ' MeanTrainScore', t_v, ' MeanTestScore: ', v_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
